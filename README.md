# triton_regressors

**GPU-accelerated implementations of classical regression models using Triton.**

This repo is an **educational + systems-focused project** that reimplements the **inference path** of classic scikit-learn regression models using **Triton GPU kernels**, with a strong emphasis on:

- performance
- correctness
- memory access patterns
- real-world inference tradeoffs

This is **not** a drop-in replacement for scikit-learn.

---

## ğŸ§  Motivation

Most production ML systems still rely heavily on **classical models** (e.g. linear regression, ridge, elastic net), but these models are almost always run on CPU.

Meanwhile:
- GPUs sit idle
- inference latency matters
- batching patterns vary wildly

This project explores:
> **When does it actually make sense to run classical ML inference on GPUs?**

The long-term goal is to build intuition and tooling relevant to:
- NVIDIA Triton
- Forest Inference Library (FIL)
- GPU inference systems beyond neural networks

---

## ğŸ¯ Project Scope

### What this repo **does**
- Reimplements **inference-only** versions of classic regression models
- Uses **Triton** for GPU kernels
- Compares:
  - output correctness vs scikit-learn
  - latency and throughput vs CPU
- Provides benchmarks across batch sizes

### What this repo **does NOT do**
- âŒ Training on GPU
- âŒ Replace scikit-learn
- âŒ Support sparse inputs
- âŒ Handle NaNs or missing values
- âŒ Support float64 (v1)

These are **explicit non-goals**.

---

## ğŸ“¦ Models (Planned)

| Model | Status |
|-----|------|
| LinearRegression | ğŸš§ In progress |
| Ridge | â³ Planned |
| Lasso (inference) | â³ Planned |
| ElasticNet | â³ Planned |
| Polynomial Regression | â³ Planned |

---

## ğŸ”¬ Design Philosophy

### 1. Inference-first
Training is done using **scikit-learn on CPU**.  
Only trained parameters (`coef_`, `intercept_`) are passed to Triton.

### 2. Math-driven, not sklearn-driven
We do **not** mirror scikit-learn internals.

Instead:
- define the math contract
- implement it from scratch
- use scikit-learn only as a **numerical oracle**

### 3. Explicit constraints
- float32 only
- dense inputs only
- deterministic outputs

This keeps the focus on **systems-level clarity**.

---

## ğŸ—‚ï¸ Repository Structure

```text
triton_regressors/
â”œâ”€â”€ triton_regressors/
â”‚   â”œâ”€â”€ linear/
â”‚   â”‚   â”œâ”€â”€ kernels.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â””â”€â”€ reference.py
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ sklearn_train.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ validation.py
â”‚       â””â”€â”€ timers.py
â”‚
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ linear_vs_sklearn.py
â”‚   â””â”€â”€ batch_sweep.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_linear_correctness.py
â”‚   â””â”€â”€ test_shapes.py
â”‚
â””â”€â”€ README.md
```

---

## ğŸ” Typical Workflow

1. **Train on CPU (scikit-learn)**
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression().fit(X, y)
```

2. **Extract weights**
```python
W = model.coef_
b = model.intercept_
```

3. **Move to GPU**
```python
W_t = torch.tensor(W, device="cuda", dtype=torch.float32)
b_t = torch.tensor(b, device="cuda", dtype=torch.float32)
X_t = torch.tensor(X, device="cuda", dtype=torch.float32)
```

4. **Run Triton inference**
```python
from triton_regressors.linear import TritonLinearRegression

triton_model = TritonLinearRegression(W_t, b_t)
y_pred = triton_model.predict(X_t)
```

5. **Validate correctness**
```python
np.allclose(y_pred.cpu().numpy(), model.predict(X), atol=1e-5)
```

---

## ğŸ“Š Benchmarks

Benchmarks focus on:
- latency (not just throughput)
- batch size crossover points
- GPU launch overhead vs compute

Example:
```bash
python benchmarks/linear_vs_sklearn.py
```

---

## ğŸ§ª Testing

Run:
```bash
pytest tests/
```

---

## ğŸš§ Status

This repo is under **active development**.  
APIs may change as models and kernels evolve.

---

## ğŸ“œ License
MIT
